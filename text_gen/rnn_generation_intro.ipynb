{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generation:  An Introduction\n",
    "\n",
    "My investigations of current machine learning techniques have finally brought me into the word of artifical neural networks.  I was attracted immediately to recurrent neural networks(RNNs) after reading Andrej Karpathy's [great blog on text generation with RNNs.](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)  There is a word Andrej uses in his blog to describe a part of what RNNs can do that has stuck with me: magic.  What we will be seeing in this blog is that we can train a neural network to write new text(words, senetences, paragraphs, even an entire corpus).  It this process of teaching a machine to create that still fills me with wonder and a sense of magic.  \n",
    "\n",
    "\n",
    "### Where is all this going?\n",
    "When I first learned of machine learning and artificial neural networks it invoked in me the themes of science fiction, a genre that has consistently been warning us of the looming dangers of artificial intelligence.  Although the models described in this blog are infantile when compared to a sentient, artifical creation, we should not be quick to distract ourselves: the seeds to more advanced forms of artifical intelligence start here.  Some may say true, artifical sentience is not possible and say any moral obligations to that improbable outcome can be ignored, but this is an outlook I do not share.  Yes, there is magic in what a recurrent neural network can do, but I also feel an imperative to consider the long term impacts of where all of this playful tinkering is leading us.  I have no definitive statement on the ethical and moral conduct of the current generation of data scientists and machine learning engineers.  All I ask is that we consider the long term effects, whatever they may be, of the seeds we are sowing here.  Not be overly dramatic, but who amongst us really want to be the next Oppenheimer, painfully letting out the words, \"Now I am become death, the destroyer of worlds\".\n",
    "\n",
    "## What is a recurrent neural network?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import csv as csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as py\n",
    "import re, pickle, sys, os, datetime\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import KFold\n",
    "import sklearn.metrics as skmetrics\n",
    "\n",
    "# NN Modules\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# My modules\n",
    "sys.path.insert(0,\"/Users/HAL3000/Dropbox/coding/my_modules/\")\n",
    "import keras_modules as my_keras_modules\n",
    "import w2v_modules as my_w2v_modules\n",
    "import text_modules as my_text_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  All tuneable parameters\n",
    "\n",
    "# Training Book List\n",
    "book_dir   = '/Users/HAL3000/Dropbox/coding/neural_nets/data/books/'\n",
    "input_file = book_dir+'Alice_In_Wonderland.txt'\n",
    "\n",
    "### Charachters to use as RNN features\n",
    "keep_list = '[^A-Z^a-z,.\"!? ]'\n",
    "keep_upper = True\n",
    "#keep_upper = False\n",
    "\n",
    "# Sequence length for RNN\n",
    "SEQ_LENGTH = 100\n",
    "\n",
    "# Generated Text Length\n",
    "LENGTH = 400\n",
    "\n",
    "### RNN Parameters ###\n",
    "#retrain = False\n",
    "retrain = True\n",
    "\n",
    "HIDDEN_UNITS  = 10\n",
    "HIDDEN_LAYERS = 1\n",
    "DROPOUT       = 0.05\n",
    "EPOCHS        = 5\n",
    "BATCH_SIZE    = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Text File: /Users/HAL3000/Dropbox/coding/neural_nets/data/books/Alice_In_Wonderland.txt\n",
      "\n",
      "Cleaning Raw Text...\n",
      "\n",
      " Snippet of Raw Text: ['\\ufeff', 'C', 'H', 'A', 'P', 'T', 'E', 'R', ' ', 'I', '.', ' ', 'D', 'o', 'w', 'n', ' ', 't', 'h', 'e', ' ', 'R', 'a', 'b', 'b', 'i', 't', '-', 'H', 'o', 'l', 'e', '\\n', '\\n', 'A', 'l', 'i', 'c', 'e', ' ', 'w', 'a', 's', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 'v', 'e', 'r', 'y', ' ', 't', 'i', 'r', 'e', 'd', ' ', 'o', 'f', ' ', 's', 'i', 't', 't', 'i', 'n', 'g', ' ', 'b', 'y', ' ', 'h', 'e', 'r', ' ', 's', 'i', 's', 't', 'e', 'r', ' ', 'o', 'n', ' ']\n",
      "\n",
      " Snippet of Cleaned Text: ['C', 'H', 'A', 'P', 'T', 'E', 'R', ' ', 'I', '.', ' ', 'D', 'o', 'w', 'n', ' ', 't', 'h', 'e', ' ', 'R', 'a', 'b', 'b', 'i', 't', 'H', 'o', 'l', 'e', 'A', 'l', 'i', 'c', 'e', ' ', 'w', 'a', 's', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 'v', 'e', 'r', 'y', ' ', 't', 'i', 'r', 'e', 'd', ' ', 'o', 'f', ' ', 's', 'i', 't', 't', 'i', 'n', 'g', ' ', 'b', 'y', ' ', 'h', 'e', 'r', ' ', 's', 'i', 's', 't', 'e', 'r', ' ', 'o', 'n', ' ', 't', 'h', 'e', 'b']\n",
      "\n",
      " Vocab List(58 Unique Characters):\n",
      " [' ', '!', '\"', ',', '.', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "Converting Text to Keras RNN Input Format with Sequence of 100\n",
      "Vocab length: 58\n",
      "Total Characters:  136867\n",
      "Number of Sequences: 136767\n",
      "Vectorization...\n",
      "\n",
      "Shape of input X Data: 136767 100 58\n",
      "Shape of input Y Data: 136767 58\n"
     ]
    }
   ],
   "source": [
    "# Import the desired text(will split by charachter)\n",
    "input_text = my_text_modules.input_text(input_file)\n",
    "\n",
    "# Preprocess: lower case conversion, strip useless charachters, etc.\n",
    "cleaned_text = my_text_modules.clean_text(input_text, keep_list, keep_upper)\n",
    "\n",
    "# Define the unique charachter(feature or vocab) set.\n",
    "vocab, n_vocab, ix_to_vocab, vocab_to_ix = my_text_modules.build_vocab(cleaned_text)\n",
    "\n",
    "# Build the sequences that Keras RNN will train on.  Format for input is:\n",
    "# (number_of_sequences, length_of_sequence, number_of_features)\n",
    "x_train, y_train = my_text_modules.text_to_KerasRnn_input(cleaned_text, vocab, n_vocab, SEQ_LENGTH, vocab_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                2760      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 58)                638       \n",
      "=================================================================\n",
      "Total params: 3,398\n",
      "Trainable params: 3,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_LSTM_model(x_train, n_vocab, DROPOUT=0.1, HIDDEN_LAYERS=1, HIDDEN_UNITS=10):\n",
    "    model = Sequential()\n",
    "    if HIDDEN_LAYERS == 1:\n",
    "        model.add(LSTM(HIDDEN_UNITS, input_shape=(x_train.shape[1], n_vocab)))\n",
    "    else:\n",
    "        model.add(LSTM(HIDDEN_UNITS, input_shape=(x_train.shape[1], n_vocab), return_sequences=True))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    for layer in range(HIDDEN_LAYERS-1):\n",
    "        model.add(LSTM(HIDDEN_UNITS))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(n_vocab, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "my_model = build_LSTM_model(x_train, n_vocab, DROPOUT, HIDDEN_LAYERS, HIDDEN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint and callbacks\n",
    "mydir = datetime.datetime.now().strftime('%m-%d_%H-%M')\n",
    "os.makedirs(\"logs/\"+mydir+\"/\")\n",
    "filepath = \"logs/\"+mydir+\"/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./logs/fake_news/{}'.format(time()),\n",
    "                                          histogram_freq=0, write_graph=True,\n",
    "                                          write_images=False)\n",
    "callbacks_list = [checkpoint, tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==== Training Keras NN ====\n",
      "Epochs: 5 \n",
      "batch size: 128\n",
      "Epoch 1/5\n",
      "136704/136767 [============================>.] - ETA: 0s - loss: 3.0987Epoch 00001: loss improved from inf to 3.09869, saving model to logs/12-04_15-08/weights-improvement-01-3.0987.hdf5\n",
      "136767/136767 [==============================] - 125s 917us/step - loss: 3.0987\n",
      "Epoch 2/5\n",
      "136704/136767 [============================>.] - ETA: 0s - loss: 2.9045Epoch 00002: loss improved from 3.09869 to 2.90438, saving model to logs/12-04_15-08/weights-improvement-02-2.9044.hdf5\n",
      "136767/136767 [==============================] - 124s 909us/step - loss: 2.9044\n",
      "Epoch 3/5\n",
      "136704/136767 [============================>.] - ETA: 0s - loss: 2.6981Epoch 00003: loss improved from 2.90438 to 2.69800, saving model to logs/12-04_15-08/weights-improvement-03-2.6980.hdf5\n",
      "136767/136767 [==============================] - 121s 886us/step - loss: 2.6980\n",
      "Epoch 4/5\n",
      "136704/136767 [============================>.] - ETA: 0s - loss: 2.5461Epoch 00004: loss improved from 2.69800 to 2.54621, saving model to logs/12-04_15-08/weights-improvement-04-2.5462.hdf5\n",
      "136767/136767 [==============================] - 125s 915us/step - loss: 2.5462\n",
      "Epoch 5/5\n",
      "136704/136767 [============================>.] - ETA: 0s - loss: 2.4635Epoch 00005: loss improved from 2.54621 to 2.46345, saving model to logs/12-04_15-08/weights-improvement-05-2.4635.hdf5\n",
      "136767/136767 [==============================] - 128s 935us/step - loss: 2.4635\n"
     ]
    }
   ],
   "source": [
    "# Fit the RNN\n",
    "if not os.path.exists('weights/rnn_alice.h5') or retrain:\n",
    "    print('\\n ==== Training Keras NN ====')\n",
    "    print('Epochs:', EPOCHS, '\\nbatch size:', BATCH_SIZE)\n",
    "    my_model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list)\n",
    "    my_model.save('weights/rnn_alice.h5')\n",
    "else:\n",
    "    print('\\nImporting Keras NN Model...')\n",
    "    my_model = load_model('logs/weights-improvement-02-1.8823.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating New Text of Length 400 \n",
      "\n",
      "\n",
      "----- Generating with seed: \" little sisters, the Dormouse beganin a great hurry and their names were Elsie, Lacie, and Tillie an\"\n",
      "----- End Seed -----\n",
      " little sisters, the Dormouse beganin a great hurry and their names were Elsie, Lacie, and Tillie ang, mopd thea, bt cee Kat shee otapo uttebhir tibb. Toun nolaf aeragt go Oep er Arepeshel, woge coo s uwthe nabid saner t oucept and fad aregIsd, iord nhern t eside dunte atfr bhizd hithid Varoulmlnd hicoant,re urdol Bhewev., nve whsh anshecert alile thiz?cas cin! outllid! Wiedee Yofs kothwcher  roumeorermowert an An ou whd tand hiict he of IinImT isllad set uroud Coup!s, I s ul wal gor r  sild aly\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Generate some new text(trained RNN model, desried new text length, vocab size, two vocab dicts)\n",
    "my_text_modules.generate_text(my_model, cleaned_text, LENGTH, SEQ_LENGTH,\n",
    "                              n_vocab, ix_to_vocab, vocab_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define list of models to compare\n",
    "model_list = ['logs/'+mydir+'/weights-improvement-01-3.0970.hdf5',\n",
    "              'logs/'+mydir+'/weights-improvement-03-2.6688.hdf5',\n",
    "              'logs/'+mydir+'/weights-improvement-05-2.4788.hdf5',\n",
    "             ]\n",
    "\n",
    "seed_text = \"She got to the part about her repeating YOU ARE OLD, FATHER WILLIAM, to the Caterpillar \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sentence' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-18ecf5f19d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_text_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text_diff_complexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/coding/my_modules/text_modules.py\u001b[0m in \u001b[0;36mgenerate_text_diff_complexity\u001b[0;34m(model_list, seed_text, length, seq_length, n_vocab, ix_to_vocab, vocab_to_ix)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mgen_text_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nGenerating with seed: \"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sentence' referenced before assignment"
     ]
    }
   ],
   "source": [
    "my_text_modules.generate_text_diff_complexity(model_list, seed_text, LENGTH, SEQ_LENGTH, n_vocab, ix_to_vocab, vocab_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
